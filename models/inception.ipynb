{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209566ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/szagoruyko/pytorchviz.git@master\n",
      "  Cloning https://github.com/szagoruyko/pytorchviz.git (to revision master) to /tmp/pip-req-build-ldoee3wn\n",
      "  Running command git clone -q https://github.com/szagoruyko/pytorchviz.git /tmp/pip-req-build-ldoee3wn\n",
      "Requirement already satisfied, skipping upgrade: graphviz in /home/light5551/.local/lib/python3.8/site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied, skipping upgrade: torch in /home/light5551/.local/lib/python3.8/site-packages (from torchviz==0.0.2) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66 in /home/light5551/.local/lib/python3.8/site-packages (from torch->torchviz==0.0.2) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/light5551/.local/lib/python3.8/site-packages (from torch->torchviz==0.0.2) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99 in /home/light5551/.local/lib/python3.8/site-packages (from torch->torchviz==0.0.2) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/light5551/.local/lib/python3.8/site-packages (from torch->torchviz==0.0.2) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96 in /home/light5551/.local/lib/python3.8/site-packages (from torch->torchviz==0.0.2) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchviz==0.0.2) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->torchviz==0.0.2) (45.2.0)\n",
      "Building wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4976 sha256=a3860913fb6e56034b07221af20fa4f3188fe90ddc73d18520747ea3a9f724da\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-uj8xey07/wheels/50/d5/0c/a58eabcbe90646a0801d1ed8011e1e517f6160bb03d22be4e7\n",
      "Successfully built torchviz\n",
      "Installing collected packages: torchviz\n",
      "  Attempting uninstall: torchviz\n",
      "    Found existing installation: torchviz 0.0.2\n",
      "    Uninstalling torchviz-0.0.2:\n",
      "      Successfully uninstalled torchviz-0.0.2\n",
      "Successfully installed torchviz-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall -U git+https://github.com/szagoruyko/pytorchviz.git@master\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot, make_dot_from_trace\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmagnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Magnet\n",
      "File \u001b[0;32m~/osll/micro-movement-amplification/models/magnet.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy2cuda\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtruncated_normal_\u001b[39m(tensor, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      7\u001b[0m     size \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "%pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\n",
    "from torchviz import make_dot, make_dot_from_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d20c8227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MagNet(\n",
       "  (encoder): Encoder(\n",
       "    (cba_1): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "      (conv2d): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (cba_2): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (resblks): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (texture_cba): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (texture_resblks): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (motion_cba): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (motion_resblks): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (manipulator): Manipulator(\n",
       "    (g): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (h_conv): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (h_resblk): ResBlk(\n",
       "      (cba_1): Conv2D_activa(\n",
       "        (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (cba_2): Conv2D_activa(\n",
       "        (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (texture_up): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
       "    (resblks): Sequential(\n",
       "      (0): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlk(\n",
       "        (cba_1): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (cba_2): Conv2D_activa(\n",
       "          (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
       "    (cba_1): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (cba_2): Conv2D_activa(\n",
       "      (pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "      (conv2d): Conv2d(32, 3, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magnet import MagNet\n",
    "magnet_net = MagNet()\n",
    "magnet_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "132ec9d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m384\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m make_dot(\u001b[43mmagnet_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevaluate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(magnet_net\u001b[38;5;241m.\u001b[39mnamed_parameters()))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/osll/micro-movement-amplification/models/magnet.py:177\u001b[0m, in \u001b[0;36mMagNet.forward\u001b[0;34m(self, batch_A, batch_B, batch_C, batch_M, amp_factor, mode)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_hat, texture_AC, texture_BM, motion_BC\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     texture_A, motion_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     texture_B, motion_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(batch_B)\n\u001b[1;32m    179\u001b[0m     motion_mag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanipulator(motion_A, motion_B, amp_factor)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/osll/micro-movement-amplification/models/magnet.py:92\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcba_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcba_2(x)\n\u001b[1;32m     94\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblks(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/osll/micro-movement-amplification/models/magnet.py:34\u001b[0m, in \u001b[0;36mConv2D_activa.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding:\n\u001b[0;32m---> 34\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d(x)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/padding.py:178\u001b[0m, in \u001b[0;36m_ReflectionPadNd.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(384, 384)\n",
    "\n",
    "make_dot(magnet_net(x, x, x, x, 0.5, mode='evaluate'), params=dict(magnet_net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54aa212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
